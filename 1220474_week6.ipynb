{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhEAXuKa6ntuI9qNYsFdHj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElyorS/AI-application-system/blob/main/12204556_week6_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TASK 2.1***\n",
        "==========="
      ],
      "metadata": {
        "id": "lVfRa_1rUiwI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzT9HqRTSMMZ",
        "outputId": "040dad2c-c2cb-40d4-c778-9b64085ae8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 87s 45ms/step - loss: 0.2021 - accuracy: 0.9388\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 78s 42ms/step - loss: 0.0908 - accuracy: 0.9725\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 80s 43ms/step - loss: 0.0744 - accuracy: 0.9778\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0648 - accuracy: 0.9800\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 76s 41ms/step - loss: 0.0589 - accuracy: 0.9826\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0438 - accuracy: 0.9862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0438 - accuracy: 0.9862\n",
            "Test accuracy: 0.9861999750137329\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the images\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Build the CNN model with Dropout and Batch Normalization\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.BatchNormalization(),  # Add Batch Normalization\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),  # Add Dropout\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),  # Add Batch Normalization\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),  # Add Dropout\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),  # Add Batch Normalization\n",
        "    tf.keras.layers.Dropout(0.5),  # Add Dropout\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "# Save the model\n",
        "model.save('mnist_cnn_model_with_dropout_bn.h5')\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('mnist_cnn_model_with_dropout_bn.h5')\n",
        "\n",
        "# Evaluate the loaded model\n",
        "test_loss, test_acc = loaded_model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, I implemented a Convolutional Neural Network (CNN) for classifying handwritten digits from the MNIST dataset. It incorporates Dropout and Batch Normalization layers to improve the model's performance. Dropout helps prevent overfitting by randomly deactivating neurons during training, while Batch Normalization normalizes the inputs of each layer for faster and more stable training. The code trains the model, evaluates its accuracy on a test dataset, and saves the trained model for future use. These enhancements can lead to better generalization and faster training for the CNN model.\n",
        "\n",
        "Test accuracy: 0.9861999750137329"
      ],
      "metadata": {
        "id": "eiNt9hA2UPOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2.2**\n",
        "========="
      ],
      "metadata": {
        "id": "O1zk4POcVT-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the images\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define a list of activation functions to experiment with\n",
        "activation_functions = ['sigmoid', 'tanh', 'relu']\n",
        "\n",
        "for activation_function in activation_functions:\n",
        "    # Build the CNN model with the specified activation function\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation=activation_function, input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation=activation_function),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=activation_function),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "    print(f\"Activation Function: {activation_function}\")\n",
        "    print(\"Test accuracy:\", test_acc)\n",
        "    print(\"------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgczPgjgVTjL",
        "outputId": "5c255a5b-42e4-4515-882c-e67e87505ff6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 51s 26ms/step - loss: 0.5286 - accuracy: 0.8270\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0903 - accuracy: 0.9732\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0607 - accuracy: 0.9818\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0465 - accuracy: 0.9860\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0363 - accuracy: 0.9890\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0395 - accuracy: 0.9861\n",
            "Activation Function: sigmoid\n",
            "Test accuracy: 0.9861000180244446\n",
            "------------------\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 51s 26ms/step - loss: 0.1188 - accuracy: 0.9642\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0433 - accuracy: 0.9870\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0301 - accuracy: 0.9902\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0220 - accuracy: 0.9929\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0166 - accuracy: 0.9948\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0417 - accuracy: 0.9870\n",
            "Activation Function: tanh\n",
            "Test accuracy: 0.9869999885559082\n",
            "------------------\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 48s 25ms/step - loss: 0.1307 - accuracy: 0.9601\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0441 - accuracy: 0.9858\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 46s 24ms/step - loss: 0.0295 - accuracy: 0.9907\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0208 - accuracy: 0.9935\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0151 - accuracy: 0.9951\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0278 - accuracy: 0.9908\n",
            "Activation Function: relu\n",
            "Test accuracy: 0.9908000230789185\n",
            "------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test accuracy with sigmoid: 0.9861000180244446\n",
        "_____________________________________\n",
        "Test accuracy with tanh: 0.9869999885559082\n",
        "_____________________________________\n",
        "Test accuracy with relu: 0.9908000230789185"
      ],
      "metadata": {
        "id": "Iboim-jgZqzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The impact of activation functions on CNN performance varies. Sigmoid and tanh have been used historically but can suffer from the vanishing gradient problem. ReLU (Rectified Linear Unit) is a popular choice as it tends to lead to faster convergence and often achieves better results. However, it's important to experiment with different activation functions to see which one works best for a particular problem. In this case, relu had the best accurasy, while tanh had the 2nd best accurasy. Sigmoid has lower accurasy compared to relu and tanh."
      ],
      "metadata": {
        "id": "aTYXTxSrVrDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TASK 2.3***\n",
        "========\n"
      ],
      "metadata": {
        "id": "5ZN0hGLnV3Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the images\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Build the CNN model with more convolutional and pooling layers\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),  # Add another convolutional layer\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1sPtNqIV6qY",
        "outputId": "802c0ae7-f0d2-420a-9eb2-b3adbcb6fced"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 52s 27ms/step - loss: 0.1870 - accuracy: 0.9420\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0671 - accuracy: 0.9798\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0478 - accuracy: 0.9851\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.0383 - accuracy: 0.9879\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0293 - accuracy: 0.9909\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0559 - accuracy: 0.9839\n",
            "Test accuracy: 0.9839000105857849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I added an additional convolutional layer with 128 filters and another max-pooling layer. This makes the model deeper, allowing it to learn more complex features. The impact of adding more layers will depend on the specific dataset and problem. Deeper networks can potentially recognize more intricate patterns but may also require more data and careful regularization to prevent overfitting.\n",
        "\n",
        "Test accuracy: 0.9839000105857849"
      ],
      "metadata": {
        "id": "I-gtLyNiWbng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TASK 2.4***\n",
        "======"
      ],
      "metadata": {
        "id": "u2Skf5x0Wlx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the images\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define a list of optimizers to experiment with\n",
        "optimizers = ['sgd', 'rmsprop', 'adam']\n",
        "\n",
        "for optimizer in optimizers:\n",
        "    # Build the CNN model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model with the specified optimizer\n",
        "    if optimizer == 'sgd':\n",
        "        optimizer_obj = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        optimizer_obj = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "    else:  # Default to Adam\n",
        "        optimizer_obj = 'adam'\n",
        "\n",
        "    model.compile(optimizer=optimizer_obj,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "    print(f\"Optimizer: {optimizer}\")\n",
        "    print(\"Test accuracy:\", test_acc)\n",
        "    print(\"------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n-3ZxgQWqGp",
        "outputId": "72a64665-b816-4cac-e5ee-45c022bc1057"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.5209 - accuracy: 0.8536\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 46s 24ms/step - loss: 0.1379 - accuracy: 0.9586\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 46s 24ms/step - loss: 0.0965 - accuracy: 0.9704\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 46s 25ms/step - loss: 0.0767 - accuracy: 0.9766\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0654 - accuracy: 0.9795\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0577 - accuracy: 0.9804\n",
            "Optimizer: sgd\n",
            "Test accuracy: 0.980400025844574\n",
            "------------------\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 46s 24ms/step - loss: 0.1229 - accuracy: 0.9617\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 46s 25ms/step - loss: 0.0411 - accuracy: 0.9874\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.0288 - accuracy: 0.9917\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0220 - accuracy: 0.9933\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0174 - accuracy: 0.9947\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0300 - accuracy: 0.9915\n",
            "Optimizer: rmsprop\n",
            "Test accuracy: 0.9915000200271606\n",
            "------------------\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.1290 - accuracy: 0.9601\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 46s 24ms/step - loss: 0.0426 - accuracy: 0.9868\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0295 - accuracy: 0.9908\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 46s 25ms/step - loss: 0.0201 - accuracy: 0.9937\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 46s 25ms/step - loss: 0.0149 - accuracy: 0.9951\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0268 - accuracy: 0.9915\n",
            "Optimizer: adam\n",
            "Test accuracy: 0.9915000200271606\n",
            "------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test accurasy with sgd: 0.980400025844574\n",
        "_________________\n",
        "Test accurasy with rmsprop: 0.9915000200271606\n",
        "_________________\n",
        "Test accurasy with adam: 0.9915000200271606"
      ],
      "metadata": {
        "id": "87t9GfCecHze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code I iterated through three different optimizers (SGD, RMSprop, and Adam), train a separate CNN model for each optimizer, and evaluate and print the test accuracy. The choice of optimizer can significantly affect training speed and performance. SGD is a basic optimizer that can work well with proper tuning but may converge slowly. RMSprop adapts the learning rate per parameter and can be faster in some cases. Adam is a popular choice due to its adaptive learning rates and often provides faster convergence and better performance. However, the optimal optimizer can vary depending on the specific dataset and problem, so experimentation is key. In this sneario, rmsprop and adam had a better performance compared to sgd."
      ],
      "metadata": {
        "id": "Dv5k00TUXyop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TASK 2.5***\n",
        "=========\n"
      ],
      "metadata": {
        "id": "oiMJqvlqYI5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load Data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# Split data into train and test\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "model = tf . keras . Sequential ([\n",
        "    tf.keras.layers.Conv2D(32 , (3,3) , activation ='relu',input_shape =(28 , 28 , 1) ) ,\n",
        "    tf.keras.layers.MaxPooling2D ((2 , 2) ) ,\n",
        "    tf.keras.layers.Conv2D(64 , (3 ,3) , activation ='relu') ,\n",
        "    tf.keras.layers.MaxPooling2D((2 , 2) ) ,\n",
        "    tf.keras.layers.Flatten() ,\n",
        "    tf.keras.layers.Dense(128 , activation ='relu') ,\n",
        "    tf.keras.layers.Dense(10 , activation ='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile( optimizer ='adam' ,loss = 'sparse_categorical_crossentropy',metrics = [ 'accuracy'])\n",
        "model.fit( train_images , train_labels , epochs =5)\n",
        "\n",
        "test_loss , test_acc = model . evaluate ( test_images , test_labels)\n",
        "model.save ('mnist_cnn_model.h5')\n",
        "\n",
        "loaded_model = tf.keras.models.load_model ('mnist_cnn_model.h5')\n",
        "\n",
        "test_loss , test_acc = loaded_model.evaluate (test_images ,test_labels)\n",
        "\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3JUk-czkGbI",
        "outputId": "1725c0d9-53df-4986-c598-c6b33d9f2e4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 48s 25ms/step - loss: 0.1276 - accuracy: 0.9610\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 46s 25ms/step - loss: 0.0437 - accuracy: 0.9865\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 46s 25ms/step - loss: 0.0309 - accuracy: 0.9904\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 47s 25ms/step - loss: 0.0213 - accuracy: 0.9932\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 46s 24ms/step - loss: 0.0159 - accuracy: 0.9951\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0328 - accuracy: 0.9894\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0328 - accuracy: 0.9894\n",
            "Test accuracy: 0.9894000291824341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, I evaluated my trained CNN model using the test dataset. This evaluation process helped me gauge how well my model generalizes to unseen data and provided insights into its overall performance. By loading the previously trained model and using the evaluate method, I obtained important metrics such as test loss and test accuracy.\n",
        "\n",
        "Test accuracy: 0.9894000291824341"
      ],
      "metadata": {
        "id": "o_Yk1qKMouPT"
      }
    }
  ]
}